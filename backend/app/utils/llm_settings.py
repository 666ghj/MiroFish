"""
LLM settings persisted on disk (local-only).

This module is intentionally lightweight so it can be imported by:
- Flask backend (API endpoints + services)
- simulation scripts under backend/scripts (no Flask dependency)
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

from openai import OpenAI

from ..config import Config, _normalize_openai_base_url


def _project_root() -> str:
    return os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../"))


def default_llm_settings_path() -> str:
    """
    Default local settings path (ignored by git):
    - MiroFish-config/llm.json (preferred)
    """
    return os.path.join(_project_root(), "MiroFish-config", "llm.json")


def legacy_llm_settings_path() -> str:
    """
    Legacy fallback path under uploads (ignored by git).
    """
    return os.path.join(_project_root(), "backend", "uploads", "settings", "llm.json")


def resolve_llm_settings_path() -> str:
    explicit = os.environ.get("MIROFISH_LLM_CONFIG_FILE") or os.environ.get("MIROFISH_LLM_SETTINGS_FILE")
    if explicit:
        return explicit
    preferred = default_llm_settings_path()
    legacy = legacy_llm_settings_path()
    if os.path.exists(preferred):
        return preferred
    if os.path.exists(legacy):
        return legacy
    return preferred


def _atomic_write_json(path: str, data: Dict[str, Any]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    tmp_path = f"{path}.tmp"
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp_path, path)


# Stage å®šä¹‰ï¼šç”¨äºæ¨¡å‹è·¯ç”±
STAGE_DEFINITIONS: Dict[str, Dict[str, Any]] = {
    "json_structure": {
        "label": "JSON ç»“æ„åŒ–è¾“å‡º",
        "description": "ç”¨äºå¤§çº²è§„åˆ’ã€å­é—®é¢˜ç”Ÿæˆã€é‡‡è®¿é—®é¢˜ç”Ÿæˆç­‰éœ€è¦ä¸¥æ ¼ JSON æ ¼å¼çš„ä»»åŠ¡",
        "recommended": ["gpt-5.2", "deepseek-v3.2-chat", "glm-4.7", "gemini-claude-sonnet-4-5"],
        "warnings": [
            {"pattern": "-thinking$", "message": "æ¨ç†æ¨¡å‹å¯èƒ½è¿”å›ç©º JSONï¼Œä¸å»ºè®®ç”¨äºæ­¤é˜¶æ®µ", "level": "warning"},
            {"pattern": "-reasoner$", "message": "æ¨ç†æ¨¡å‹å¯èƒ½è¿”å›ç©º JSONï¼Œä¸å»ºè®®ç”¨äºæ­¤é˜¶æ®µ", "level": "warning"},
            {"pattern": "^gemini-3-pro", "message": "å·²çŸ¥ JSON è¾“å‡ºä¸ç¨³å®šï¼Œå¼ºçƒˆä¸å»ºè®®", "level": "error"},
        ],
        "tip": "ğŸ’¡ æ¨èä½¿ç”¨ GPT-5.2ï¼ŒJSON è¾“å‡ºæœ€ç¨³å®šï¼Œtoken æ¶ˆè€—å°",
    },
    "content_generation": {
        "label": "æŠ¥å‘Šå†…å®¹ç”Ÿæˆ",
        "description": "ç”¨äºç”ŸæˆæŠ¥å‘Šç« èŠ‚çš„é•¿æ–‡æœ¬å†…å®¹ï¼Œéœ€è¦é«˜è´¨é‡çš„æ–‡å­—è¡¨è¾¾",
        "recommended": ["gemini-claude-sonnet-4-5", "gemini-claude-opus-4-5-thinking"],
        "warnings": [],
        "tip": "ğŸ’¡ æ¨èä½¿ç”¨ Claude Sonnet 4.5ï¼Œå¹³è¡¡è´¨é‡ä¸æˆæœ¬",
    },
    "reasoning": {
        "label": "å¤æ‚æ¨ç†ä»»åŠ¡",
        "description": "ç”¨äºæ·±åº¦åˆ†æã€ç­–ç•¥è§„åˆ’ç­‰éœ€è¦æ·±åº¦æ€è€ƒçš„ä»»åŠ¡",
        "recommended": ["gemini-claude-opus-4-5-thinking", "deepseek-v3.2-reasoner", "kimi-k2-thinking"],
        "warnings": [],
        "tip": "ğŸ’¡ æ¨ç†æ¨¡å‹æ“…é•¿æ·±åº¦åˆ†æï¼Œä½† token æ¶ˆè€—è¾ƒé«˜",
    },
    "profile_generation": {
        "label": "Agent äººè®¾ç”Ÿæˆ",
        "description": "ç”¨äºç”Ÿæˆæ¨¡æ‹Ÿ Agent çš„äººç‰©è®¾å®šï¼Œéœ€è¦åˆ›æ„æ€§æ–‡æœ¬",
        "recommended": ["gemini-claude-sonnet-4-5", "deepseek-v3.2-chat"],
        "warnings": [],
        "tip": "ğŸ’¡ éœ€è¦åˆ›æ„æ€§ï¼Œæ¨èç»¼åˆèƒ½åŠ›å¼ºçš„æ¨¡å‹",
    },
    "fallback": {
        "label": "é»˜è®¤/å…¶ä»–ä»»åŠ¡",
        "description": "æœªåˆ†ç±»çš„å…¶ä»–ä»»åŠ¡",
        "recommended": [],
        "warnings": [],
        "tip": "ä½¿ç”¨é»˜è®¤æ¨¡å‹",
    },
}

# é¢„è®¾æ–¹æ¡ˆ
MODEL_ROUTING_PRESETS: Dict[str, Dict[str, Any]] = {
    "economy": {
        "label": "ç»æµæ¨¡å¼",
        "description": "æˆæœ¬æœ€ä½ï¼Œé€‚åˆæµ‹è¯•",
        "routing": {
            "json_structure": "gpt-5.2",
            "content_generation": "deepseek-v3.2-chat",
            "reasoning": "deepseek-v3.2-reasoner",
            "profile_generation": "deepseek-v3.2-chat",
            "fallback": "gpt-5.2",
        },
    },
    "quality": {
        "label": "è´¨é‡ä¼˜å…ˆ",
        "description": "è´¨é‡æœ€é«˜ï¼Œæˆæœ¬è¾ƒé«˜",
        "routing": {
            "json_structure": "gpt-5.2",
            "content_generation": "gemini-claude-opus-4-5-thinking",
            "reasoning": "gemini-claude-opus-4-5-thinking",
            "profile_generation": "gemini-claude-sonnet-4-5",
            "fallback": "gpt-5.2",
        },
    },
    "balanced": {
        "label": "æ··åˆæ¨è",
        "description": "å¹³è¡¡è´¨é‡ä¸æˆæœ¬ï¼ˆé»˜è®¤ï¼‰",
        "routing": {
            "json_structure": "gpt-5.2",
            "content_generation": "gemini-claude-sonnet-4-5",
            "reasoning": "gemini-claude-opus-4-5-thinking",
            "profile_generation": "deepseek-v3.2-chat",
            "fallback": "gpt-5.2",
        },
    },
}


@dataclass(frozen=True)
class LLMSettings:
    base_url: str
    api_key: Optional[str]
    models: List[str]
    model_routing: Dict[str, str]  # stage -> model æ˜ å°„
    updated_at: Optional[str] = None
    source_path: Optional[str] = None

    def normalized_base_url(self) -> str:
        return _normalize_openai_base_url(self.base_url) if self.base_url else ""

    def get_model_for_stage(self, stage: str) -> Optional[str]:
        """è·å–æŒ‡å®š stage å¯¹åº”çš„æ¨¡å‹ï¼Œå¦‚æœæœªé…ç½®è¿”å› None"""
        return self.model_routing.get(stage) or self.model_routing.get("fallback")

    def public_dict(self) -> Dict[str, Any]:
        key = (self.api_key or "").strip()
        return {
            "base_url": self.normalized_base_url(),
            "models": self.models,
            "model_routing": self.model_routing,
            "api_key_set": bool(key),
            "api_key_last4": key[-4:] if len(key) >= 4 else (key if key else ""),
            "updated_at": self.updated_at,
            "source_path": self.source_path,
        }

    def create_openai_client(self) -> OpenAI:
        if not self.api_key:
            raise ValueError("LLM_API_KEY æœªé…ç½®ï¼ˆè¯·åœ¨è®¾ç½®é¡µå¡«å†™æˆ–åœ¨ .env ä¸­é…ç½®ï¼‰")
        base_url = self.normalized_base_url()
        if base_url:
            return OpenAI(api_key=self.api_key, base_url=base_url)
        return OpenAI(api_key=self.api_key)


def load_llm_settings() -> LLMSettings:
    """
    Load settings from disk, falling back to environment.
    """
    path = resolve_llm_settings_path()
    data: Dict[str, Any] = {}
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f) or {}
        except Exception:
            data = {}

    base_url = (data.get("base_url") or "").strip() or (Config.LLM_BASE_URL or "").strip()
    api_key = (data.get("api_key") or "").strip() or (Config.LLM_API_KEY or "").strip() or None

    models: List[str] = []
    raw_models = data.get("models")
    if isinstance(raw_models, list):
        models = [str(m).strip() for m in raw_models if str(m).strip()]
    if not models:
        model = (data.get("model") or "").strip() or (Config.LLM_MODEL_NAME or "").strip()
        if model:
            models = [model]

    # è¯»å– model_routing é…ç½®
    model_routing: Dict[str, str] = {}
    raw_routing = data.get("model_routing")
    if isinstance(raw_routing, dict):
        for stage, model_name in raw_routing.items():
            if isinstance(stage, str) and isinstance(model_name, str) and model_name.strip():
                model_routing[stage.strip()] = model_name.strip()

    updated_at = data.get("updated_at") if isinstance(data.get("updated_at"), str) else None

    return LLMSettings(
        base_url=base_url,
        api_key=api_key,
        models=models[:10],
        model_routing=model_routing,
        updated_at=updated_at,
        source_path=path,
    )


def save_llm_settings(
    *,
    base_url: Optional[str] = None,
    api_key: Optional[str] = None,
    models: Optional[List[str]] = None,
    model_routing: Optional[Dict[str, str]] = None,
    clear_api_key: bool = False,
) -> LLMSettings:
    """
    Persist settings to disk.

    Notes:
    - Settings are local-only. The default path is ignored by git.
    - `api_key` is stored only when provided; use `clear_api_key=True` to remove.
    - `model_routing` maps stage names to model names.
    """
    current = load_llm_settings()
    next_base_url = (base_url if base_url is not None else current.base_url).strip()
    next_base_url = _normalize_openai_base_url(next_base_url) if next_base_url else ""

    if clear_api_key:
        next_api_key: Optional[str] = None
    elif api_key is not None:
        next_api_key = api_key.strip() or None
    else:
        next_api_key = current.api_key

    next_models = current.models
    if models is not None:
        next_models = [str(m).strip() for m in models if str(m).strip()]
        next_models = next_models[:10]

    # å¤„ç† model_routing
    next_routing = dict(current.model_routing)
    if model_routing is not None:
        # åˆå¹¶æ–°é…ç½®ï¼ˆå…è®¸éƒ¨åˆ†æ›´æ–°ï¼‰
        for stage, model_name in model_routing.items():
            if isinstance(stage, str) and stage.strip():
                if model_name and isinstance(model_name, str) and model_name.strip():
                    next_routing[stage.strip()] = model_name.strip()
                elif stage.strip() in next_routing:
                    # ç©ºå€¼è¡¨ç¤ºåˆ é™¤è¯¥ stage é…ç½®
                    del next_routing[stage.strip()]

    payload: Dict[str, Any] = {
        "base_url": next_base_url,
        "api_key": next_api_key or "",
        "models": next_models,
        "model_routing": next_routing,
        "updated_at": datetime.now().isoformat(),
    }

    path = resolve_llm_settings_path()
    _atomic_write_json(path, payload)
    return load_llm_settings()
